% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/active_learning.R
\name{ScoreRBMActiveLearningCandidates}
\alias{ScoreRBMActiveLearningCandidates}
\title{Active Learning Scoring for RBM Candidate Cells}
\usage{
ScoreRBMActiveLearningCandidates(
  rbmObject,
  candidateExpression,
  method = c("expected_gradient", "latent_entropy"),
  aggregate = c("sum", "max"),
  transform = c("none", "log1p"),
  chunkSize = 1000,
  parallel = FALSE,
  numWorkers = NULL,
  progressr = TRUE,
  verbose = TRUE
)
}
\arguments{
\item{rbmObject}{An RBM object fitted using `FitRBM()`.}

\item{candidateExpression}{A numeric matrix (or sparse `Matrix`) of expression values
with rows as features and columns as candidate observations. Row names must be
feature names. Missing RBM features are treated as zero.}

\item{method}{Scoring method: `"expected_gradient"` or `"latent_entropy"`.}

\item{aggregate}{How to aggregate across hidden layers: `"sum"` (default) or `"max"`.}

\item{transform}{Optional transform applied to candidate expression before scoring:
`"none"` (default) or `"log1p"`.}

\item{chunkSize}{Integer, number of candidate columns to score per chunk (default: 1000).}

\item{parallel}{Logical; if `TRUE`, uses `future.apply::future_lapply()` over chunks.}

\item{numWorkers}{Optional integer number of parallel workers. If `NULL`, uses
`parallel::detectCores() - 1`.}

\item{progressr}{Logical indicating whether to use `progressr` progress reporting.}

\item{verbose}{Logical indicating whether to print progress messages.}
}
\value{
A numeric vector of scores (length = number of candidate observations) with
  names matching `colnames(candidateExpression)` when available.
}
\description{
Scores candidate observations (cells) for active learning by estimating the expected
impact each candidate would have on the RBM. Two scoring strategies are provided:
}
\details{
- **"expected_gradient"**: a fast proxy for expected parameter change computed from
  the conditional expectation of hidden activations under the model-induced
  conditional activation distributions (positive-phase only). For each hidden layer,
  this uses the fact that the positive-phase weight gradient for a
  single observation is proportional to \eqn{v \otimes E[h | v]}.
  We score candidates by the Frobenius norm of this outer product, which equals
  \eqn{||v||_2 \cdot ||E[h | v]||_2}.

- **"latent_entropy"**: total entropy of the hidden-layer conditional activation
  distributions for the
  candidate (Bernoulli entropy for binary layers; categorical entropy for softmax
  layers). Higher entropy corresponds to higher latent uncertainty.

The implementation is chunked and can be parallelized with `future.apply`.
}
